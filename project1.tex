\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xcolor, soul}
\usepackage[english]{babel}
\usepackage[margin=1.7in]{geometry}
\bibliographystyle{unsrt}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{tabularx,ragged2e,booktabs,caption,sidecap,subcaption,graphicx,tikz}

\begin{document}
\begin{titlepage}
	
	
	\title{Project 1 \\ Course 02445 \\ Project in Statistical evaluation of \\ artificial intelligence }
	\author{Rasmus J. P. s164564 \\ Nikolaj S. P. s183930}
	\date{January 2020}
	\maketitle
	
\subsection*{Summary}
Classifying trajectories is a complex problem with many dimensions. In this report we will attempt to classify trajectories with two different machine learning models, a neural network and the K-Nearest-Neighbors algorithm. We evaluated the model performances on how well they classify trajectories and subsequently predicts which test-subject performed said trajectory and compared the performance of both models. We found a significant difference between model performances in favor of the neural network $\alpha < 0.01$. In addition we analyzed 16 different experiments and their resulting trajectories and tested whether there was a significant effect of experiment on trajectories. Using a multivariate test-statistics for high dimensional data we are able to conclude that 115/120 pairs of two-sample comparisons of means were significantly different than one-another $\alpha = 0.05$.

\end{titlepage}

\section{Introduction}
Solving complex problems has been the main drive for development in computer science and the computers has by far overceeded the humans on complex problems such as playing a game of chess or predicting the weather but only because we have been able to present them models simulating the real world for which the computer can react upon. So how do we model the real world? There are many answers, some complicated and some simple. We will be looking at trajectory data from 10 different test-subjects each performing 16 different experiments, repeated 10 times. Each experiment share the same underlying task with slight variation to it. The task for the test-subjects was that they had to move an arbitrary cylinder over another cylinder. The experiments varied between different obstacle and obstacle positions. \\ Our first aim is to classify the unique trajectories from the resulting 1600 observations and evaluate the performance of our two classifiers and compare their mean squared error using two-sampled t-test. The second aim is to look for a significant effect from the experiments on the trajectories also here we will be using a form of t-test but since we are now looking at a trajectory as a whole we must use a test-statistic which takes the dimensionality into considerations, we end up comparing trajectories by using a generalized form of the Student's t-statistic named Hotelling's t-squared statistics t2 which generalizes to p-dimensionality.


\section{Data}
The trajectory data was recorded in 3 dimensions using a motion capture camera, resulting in three continuous variables x,y and z, furthermore the data included information about which person performed the motion, in which repetition the motion was captured and which experiments was performed thus giving us three categorical variables. \\ Each trajectory observation contains 100 recordings of said coordinates - see figure TRAJ.
A computer doesn't observe data like humans, so we decided to transform the motion data from 3 x 100 observations to 1 x 300, effectively stacking 300 coordinates along one vector.

Person 9 is missing some of the initial 1-4 datapoints for some of the experiments (not experiment 2), 
we impute the values with the first available datapoints, such that the first 1-5 datapoints are the same for those particular observation. This seems reasonable since it is only a few datapoints and they are located at the beginning of the observation, thus it is equivalent to the person starting from that position and holding it there for the first few measurements.

Include a few plots - (Distribution of curves HOW?)
TRAJ, variance between curves (boxplots), 

\section{Comparing classifiers}
We decided to compare two vastly different machine learning models, an ANN and i KNN, by running them multiple times and comparing the mean of our performance measure within each model. We ran each model 30 times and by doing so recording 30 independent and identically distributed random variables for each model. Central limit theorem then tells us that these random variables will follow an approximate normal distribution and because of this fact we will be able to compare the two models by comparing the mean of their performance in a two-sampled Student's t-test.

We propose the null-hypothesis \fcolorbox{gray}{gray}{$H_{0}$: The difference in the means is zero}. 
\subsection{Model A}
We experimented with several versions of ANNs to find the  architecture best suited for the task. We decided to use an ANN because of their already established performance in high dimensional space. The classification network was trained to classify a person from the 1 x 300 long vector of motion data.   


\subsection{Model B}
The second model was a clustering model using the K-Nearest-neighbor KNN algorithm, also this model was trained on the same 1 x 300 motion vector.
The KNN was choosen because of its simplicity and because it's very cheap computationally compared to other models such as the NN.
\\
Leave one out Cross Validation LOOCV was performed on both models and their performance evaluated with the zero-one loss function suitable for classification problems. NOTE: Might consider another. \hl{SVM hinge loss, TOP-k ... because multiclass}.



\section{Testing the for experimental influence}
We decided on a test statistic of difference between experiments. If the experiment were to have a significant influence on the resulting trajectories, they should all be significantly different from one-another. The multiple test statistics to asses is then a collection of 120 comparisons between each and all experiments, only concerning about unique pairs, from which we will make our conclusion upon.\\ This raises two points of interest concerning our data:
1. Can the distribution of the repititons withing experiments be considered multivariate \textbf{normal}?
2. Having only 100 observations per experiment we are limited to less than 100 explaining variables \hl{Insert a reason about Degrees of freedom here}.

A solution for the second concern is to reduce the dimensionallity by performing principal component analysis PCA and choosing the number of principal components PC's by analyzing the resulting decompisition matrix i.e variance explained. But first we must solve the  multivariate normal assumption. \\
 We have enough observations to assume normal distribution but we should only do this within single test subjects i.e. we cannot expect the motion of one subject in a test to be identical to another subject's motion and must reduce our data to 1 mean observation per subject per experiment. We have now just limited ourselves to a maximum of nine explaining variables down from 300 \hl{have we???, see later highlight}. \\
 Fortunately performing PCA returns a 82\% variance explained by the first 9 PC's and proceede to project the matrix of means onto the rotation matrix resulted from PCA. The reason for this detour is that testing for multivariate normallity returned a negative and thus we must turn to CLT.

\hl{finpuds nedenstaaende}
When testing for influence of experiments on the curves we choose a  generalized version of the two-sampled Student's t-test namely the t-squared-test ref: "Hotellinger". Thus we propose a null-hypothesis that the mean of the different trajectories are the same. The t-squared test generalizes to multiple dimensions making it a perfect statistics for our 300 dimensional means. With the t-squared test we can handle independent multivariate normal distributions by calculating the mean and standard deviation of the one 100 repetitions in each experiment and finally compare all possible sets of two experiments. This statistics deserves its own explanation which we briefly provide in the appendix.


%\begin{figure}[h]
%	\centering
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{datasets/3_GMM2.png}
%		\label{fig:density}
%	\end{subfigure}%
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{datasets/3_GMM10.png}
%		\label{fig:density10}
%	\end{subfigure}
%	\caption{Gaussian Mixture Model clusters for K=2 cluster (left pane) and K=10 cluster (right pane)}
%	\label{fig:dens}
%\end{figure}


\subsection*{Evaluating clusters}
We use Rand index to evaluate the clustering of our different methods. Specifically we will compare our true labels with the labels given by a model.
We define two measures from our clustering:
\begin{equation}
S=\sum_{i=1}^{N-1} \sum_{j=i+1}^{N} S_{i j}
\end{equation}
\begin{equation}
D=\sum_{i=1}^{N-1} \sum_{j=i+1}^{N} D_{i j}
\end{equation}
The notation requires some explanation. We diffine two cluster Z (true) and Q (predicted). $S_{ij} = 1$ if and only if  Z and Q agrees that the pair of observation ${x_i,x_j}$ belong to the same cluster, otherwise $S_{ij} = 0$. Similar  $D_{ij} = 1$ only if Z and Q agrees that the pair of observation ${x_i,x_j}$ doesn't belong to the same cluster. 
We then calculate the rand index like:
\begin{equation}
R(Q, P)=\frac{S+D}{\frac{1}{2} N(N-1)}
\end{equation}

\section{Results}
LOOCV was performed 30 times to get a good estimate of the variance of the generalization accuracy of the two classification models, from this a confidence interval was calculated \ref{tab:title}.

\begin{minipage}{\linewidth}
\centering
\captionof{table}{Confidence interval of classifiers} 
\begin{tabular}{ c c}\toprule[1.5pt]
\bf Model  & \bf CI of Generalization Accuracy \\\midrule
ANN  & $0.708 \pm 0.0078$  \\\midrule
KNN  & $0.644 \pm 0.0066$ \\
\bottomrule[1.25pt]
\end {tabular}\par
\label{tab:title}
\end{minipage} \bigskip


\begin{equation}
\begin{array}{ccc}
\hline \text { Test } & {\text { Test Statistic }} & {\text { p-value }} \\
\hline \text {Paired t-test} & {10.934} & {8e-12} \\
\hline
\end{array}
\end{equation}

\section{Appendix}
\begin{tabular}[H]{c l @{} l}
\centering
Layer no.       &
\multicolumn{2}{c}{Function} \\
\hline
Layer 1     & linear(300, 150) \\
            & ReLU \\
            & Dropout(0.15) \\
Layer 2     & linear(150, 75) \\ 
            & ReLU \\
            & Dropout(0.15) \\
Layer 3     & linear(75, 10) \\ 
Layer 4     & Softmax\\ 
\end{tabular}\\ 




\end{document}