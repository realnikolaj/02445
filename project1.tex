\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage[margin=1.7in]{geometry}
\bibliographystyle{unsrt}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{mathtools}


\begin{document}
\begin{titlepage}
	
	
	\title{Project 1 \\ Course 02445 \\ Project in Statistical evaluation of \\ artificial intelligence }
	\author{Rasmus J. P. s164564 \\ Nikolaj S. P. s183930}
	\date{January 2020}
	\maketitle
\end{titlepage}

\section{Introduction}


\section{Data}
Something general about the data

Person 9 is missing some of the initial 1-4 datapoints for some of the experiments (not experiment 2), 
we impute the values with the first available datapoints, such that the first 1-5 datapoints are the same for those particular observation. This seems reasonable since it is only a few datapoints and they are located at the beginning of the observation, thus it is equivalent to the person starting from that position and holding it there for the first few measurements.

\section{Method}
\subsection{Artificial Neural Network}
We decided to use an ANN because of the high dimensionality of the data, with one observation being describe by a 300 dimensional vector.
A classification network was trained to classify a person from the curve created by their movement in experiment 2. Several architectures where trained to find the best suited architecture for the task, in the end we decided on this particular architecture:

\begin{tabular}[H]{c l @{} l}
\centering
Layer no.       &
\multicolumn{2}{c}{Function} \\
\hline
Layer 1     & linear(300, 150) \\
            & ReLU \\
            & Dropout(0.15) \\
Layer 2     & linear(150, 75) \\ 
            & ReLU \\
            & Dropout(0.15) \\
Layer 3     & linear(75, 10) \\ 
Layer 4     & Softmax\\ 
\end{tabular}\\ 

\noindent A leave one out cross-validation was then performed on the network, to estimate it's generalization error.

\subsection{Clustering}


\begin{equation}
E=\sum_{i=1}^{N} \sum_{k=1}^{K} z_{i k}\left\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{k}\right\|_{2}^{2}
\end{equation}
\begin{equation}
z_{i k} = \frac{1}{K}
\end{equation}
Notice how $z_{ik}$ and $\mu_{k}$ will change every time a cluster is updated.
The formulae for K-means computes the sum of squares between each observation in a cluster to the cluster mean.
Ward's method or Ward linkage compares the K-means error from any giving merger between two observation or clusters. Combination of two observations or clusters are then determined by the merger with lowest increase to the K-means error.
Ward linkage then becomes the function that determines the agglomeration that creates the following dendrogram.
%\begin{figure}[H]
%	\includegraphics[width=\linewidth]{datasets/dendrogram.png}
%	\caption{Agglomerative Hierarchical clustering - dendrogram showing observation along the horizontal axis. Dissimilarity between merging clusters/observations is found as the height of where the vertical lines connect two clusters/observations.}
%	\label{fig:dendrogram}
%\end{figure}




%\begin{figure}[h]
%	\centering
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{datasets/3_GMM2.png}
%		\label{fig:density}
%	\end{subfigure}%
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{datasets/3_GMM10.png}
%		\label{fig:density10}
%	\end{subfigure}
%	\caption{Gaussian Mixture Model clusters for K=2 cluster (left pane) and K=10 cluster (right pane)}
%	\label{fig:dens}
%\end{figure}

\begin{equation}\begin{array}{lll}
		{\mu_{1}=\begin{bmatrix*}[r] -0.67 \\	0.78 \\	1.12 \\	-0.45 \\	-0.85 \\	1.18 \\	1.27 \\	0.6 \\ -0.8 \\ 	1.11 \\	1.23	\end{bmatrix*} \quad }
		{\mu_{2}=\begin{bmatrix*}[r]0.33 \\	-0.39 \\	-0.56 \\	0.22 \\	0.42 \\	-0.59 \\	-0.63 \\ 	-0.3 \\ 	0.4 \\	-0.55 \\	-0.61
				\end{bmatrix*} \quad }
		{\mu_{6}=\begin{bmatrix*}[r] -1.36 \\	2.28	\\1.13 \\	-0.58 \\	-1.04 \\	1.66 \\	1.53 \\	0.8 \\-1.89 \\ 	1.15 \\	1.44 \\
				\end{bmatrix*} \quad }
			\end{array}
\end{equation}

\begin{equation}\begin{array}{lll}
{\hat{\mu}_{1}=\begin{bmatrix*}[r]1.51	\\ 0.08
		\end{bmatrix*} \quad }
{\hat{\mu}_{2}=\begin{bmatrix*}[r]-3.16 \\	-0.17
		\end{bmatrix*} \quad }
{\hat{\mu}_{6}=\begin{bmatrix*}[r]-2.02 \\	3.45
		\end{bmatrix*} \quad }
\end{array}
\end{equation}

\subsection*{Evaluating clusters}
We use Rand index to evaluate the clustering of our different methods. Specifically we will compare our true labels with the labels given by a model.
We define two measures from our clustering:
\begin{equation}
S=\sum_{i=1}^{N-1} \sum_{j=i+1}^{N} S_{i j}
\end{equation}
\begin{equation}
D=\sum_{i=1}^{N-1} \sum_{j=i+1}^{N} D_{i j}
\end{equation}
The notation requires some explanation. We difine two cluster Z (true) and Q (predicted). $S_{ij} = 1$ if and only if  Z and Q agrees that the pair of observation ${x_i,x_j}$ belong to the same cluster, otherwise $S_{ij} = 0$. Similar  $D_{ij} = 1$ only if Z and Q agrees that the pair of observation ${x_i,x_j}$ doesn't belong to the same cluster. 
We then calculate the rand index like:
\begin{equation}
R(Q, P)=\frac{S+D}{\frac{1}{2} N(N-1)}
\end{equation}

\section{Results}

Neural network

With leave-one out cross-validation the estimated accuracy is 71\% with a 95\% confidence interval of  [79.9\% , 62.1\%]

\begin{table}[H]
\begin{tabular}{lllllllllll}
\hline
\textbf{Person}   & 1                        & 2                        & 3    & 4    & 5     & 6    & 7    & 8    & 9    & 10   \\ \hline
\textbf{Accuracy} & \multicolumn{1}{c}{50\%} & \multicolumn{1}{c}{60\%} & 60\% & 80\% & 100\% & 80\% & 70\% & 80\% & 70\% & 60\% \\ \hline
\end{tabular}
\caption{Neural network, Accuracy on each person}
\label{tab:nn}
\end{table}

\end{document}