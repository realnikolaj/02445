\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage[margin=1.7in]{geometry}
\bibliographystyle{unsrt}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{mathtools}


\begin{document}
\begin{titlepage}
	
	
	\title{Project 1 \\ Course 02445 \\ Project in Statistical evaluation of \\ artificial intelligence }
	\author{Rasmus J. P. s164564 \\ Nikolaj S. P. s183930}
	\date{January 2020}
	\maketitle
	
\subsection*{Summary}
Classifying trajectories is a complex problem with many dimensions. In this report we will attempt to classify trajectories with two different machine learning [ML] models, a neural network NN and an unsupervised clustering algorithm. We evaluated the model performance on how well they classified the trajectories and subsequently predicted which test-subject performed said trajectory and compared the performance of both models statistically. We conclude that there is not a significant difference in the performance between our neural network and the unsupervised clustering model  ... \textit{[conclusion \& results ]} ... In addition we analyzed 16 different experiments and their resulting trajectories and tested whether there was a significant effect of experiment on trajectories.  By using a multivariate test-statistics for high dimensional data we are able to conclude that different experiments results in different trajectories this with a signifance level of <1\%, we obtained a p-value for this very statistics on [result].

\end{titlepage}

\section{Introduction}
Solving complex problems has been the main drive for development in computer science and the computers has by far overceeded the humans on complex problems such as playing a game of chess or predicting the weather but only because we have been able to present them models simulating the real world for which the computer can react upon. So how do we model the real world? There are many answers, some complicated and some simple. We will be looking at trajectory data from 10 different test-subjects each performing 16 different experiments, repeated 10 times. Each experiment share the same underlying task with slight variation to it. The task for the test-subjects was that they had to move an arbitrary cylinder over another cylinder. The experiments varied between different obstacle and obstacle positions. \\ Our first aim is to classify the unique trajectories from the resulting 1600 observations and evaluate the performance of our two classifiers and compare their mean squared error using two-sampled t-test. The second aim is to look for a significant effect from the experiments on the trajectories also here we will be using a form of t-test but since we are now looking at a trajectory as a whole we must use a test-statistic which takes the dimensionality into considerations, we end up comparing trajectories by using a generalized form of the Student's t-statistic named Hotelling's t-squared statistics t2 which generalizes to p-dimensionality.


\section{Data}
The trajectory data was recorded in 3 dimensions using a motion capture camera, resulting in three continuous variables x,y and z, furthermore the data included information about which person performed the motion, in which repetition the motion was captured and which experiments was performed thus giving us three categorical variables. \\ Each trajectory observation contains 100 recordings of said coordinates - see figure TRAJ.
A computer doesn't observe data like humans, so we decided to transform the motion data from 3 x 100 observations to 1 x 300, effectively stacking 300 coordinates along one vector.

Person 9 is missing some of the initial 1-4 datapoints for some of the experiments (not experiment 2), 
we impute the values with the first available datapoints, such that the first 1-5 datapoints are the same for those particular observation. This seems reasonable since it is only a few datapoints and they are located at the beginning of the observation, thus it is equivalent to the person starting from that position and holding it there for the first few measurements.

Include a few plots - TRAJ, variance between curves (boxplots), 

\section{Classifiers}
In classifying persons from the trajectories we hypothesis that the performance of both ML models are equal and use a t-test for a two-sampled comparison of means. A brief description of both models follows and the reasoning behind choosing these excact models follows.
Several versions of ANNs where trained to find the  architecture best suited for the task. We decided to use an ANN because of their already established performance in high dimensional space. The classification network was trained to classify a person from the 1 x 300 long vector of motion data.   

Send to appendix:
\begin{tabular}[H]{c l @{} l}
\centering
Layer no.       &
\multicolumn{2}{c}{Function} \\
\hline
Layer 1     & linear(300, 150) \\
            & ReLU \\
            & Dropout(0.15) \\
Layer 2     & linear(150, 75) \\ 
            & ReLU \\
            & Dropout(0.15) \\
Layer 3     & linear(75, 10) \\ 
Layer 4     & Softmax\\ 
\end{tabular}\\ 


The second model is an unsupervised clustering model using the K-Nearest-Kneighbor KNN algorithm. This model knows not about which repetition, person or which experiment was performed it is only given the vector of 300 dimensions. 
The KNN was choosen because of its simplicity and because it's very cheap computationally compared to other models such as the NN.

Leave one out Cross validation LOOCV was performed on both models and their performance evaluated with the zero-one loss function.


\subsection{Testing for influence}
When testing for influence of experiments on the curves we choose a  generalized version of the two-sampled Student's t-test namely the t-squared-test. Thus we propose a null-hypothesis that the mean of the different trajectories are the same. The t-squared test generalizes to multiple dimensions making it a perfect statistics for our 300 dimensional means. With the t-squared test we can handle independent multivariate normal distributions by calculating the mean and standard deviation of the one 100 repetitions in each experiment and finally compare all possible sets of two experiments. This statistics deserves its own explanation which we briefly provide in the appendix.

\begin{equation}
E=\sum_{i=1}^{N} \sum_{k=1}^{K} z_{i k}\left\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{k}\right\|_{2}^{2}
\end{equation}
\begin{equation}
z_{i k} = \frac{1}{K}
\end{equation}
Notice how $z_{ik}$ and $\mu_{k}$ will change every time a cluster is updated.
The formulae for K-means computes the sum of squares between each observation in a cluster to the cluster mean.
Ward's method or Ward linkage compares the K-means error from any giving merger between two observation or clusters. Combination of two observations or clusters are then determined by the merger with lowest increase to the K-means error.
Ward linkage then becomes the function that determines the agglomeration that creates the following dendrogram.
%\begin{figure}[H]
%	\includegraphics[width=\linewidth]{datasets/dendrogram.png}
%	\caption{Agglomerative Hierarchical clustering - dendrogram showing observation along the horizontal axis. Dissimilarity between merging clusters/observations is found as the height of where the vertical lines connect two clusters/observations.}
%	\label{fig:dendrogram}
%\end{figure}




%\begin{figure}[h]
%	\centering
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{datasets/3_GMM2.png}
%		\label{fig:density}
%	\end{subfigure}%
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{datasets/3_GMM10.png}
%		\label{fig:density10}
%	\end{subfigure}
%	\caption{Gaussian Mixture Model clusters for K=2 cluster (left pane) and K=10 cluster (right pane)}
%	\label{fig:dens}
%\end{figure}

\begin{equation}\begin{array}{lll}
		{\mu_{1}=\begin{bmatrix*}[r] -0.67 \\	0.78 \\	1.12 \\	-0.45 \\	-0.85 \\	1.18 \\	1.27 \\	0.6 \\ -0.8 \\ 	1.11 \\	1.23	\end{bmatrix*} \quad }
		{\mu_{2}=\begin{bmatrix*}[r]0.33 \\	-0.39 \\	-0.56 \\	0.22 \\	0.42 \\	-0.59 \\	-0.63 \\ 	-0.3 \\ 	0.4 \\	-0.55 \\	-0.61
				\end{bmatrix*} \quad }
		{\mu_{6}=\begin{bmatrix*}[r] -1.36 \\	2.28	\\1.13 \\	-0.58 \\	-1.04 \\	1.66 \\	1.53 \\	0.8 \\-1.89 \\ 	1.15 \\	1.44 \\
				\end{bmatrix*} \quad }
			\end{array}
\end{equation}

\begin{equation}\begin{array}{lll}
{\hat{\mu}_{1}=\begin{bmatrix*}[r]1.51	\\ 0.08
		\end{bmatrix*} \quad }
{\hat{\mu}_{2}=\begin{bmatrix*}[r]-3.16 \\	-0.17
		\end{bmatrix*} \quad }
{\hat{\mu}_{6}=\begin{bmatrix*}[r]-2.02 \\	3.45
		\end{bmatrix*} \quad }
\end{array}
\end{equation}

\subsection*{Evaluating clusters}
We use Rand index to evaluate the clustering of our different methods. Specifically we will compare our true labels with the labels given by a model.
We define two measures from our clustering:
\begin{equation}
S=\sum_{i=1}^{N-1} \sum_{j=i+1}^{N} S_{i j}
\end{equation}
\begin{equation}
D=\sum_{i=1}^{N-1} \sum_{j=i+1}^{N} D_{i j}
\end{equation}
The notation requires some explanation. We difine two cluster Z (true) and Q (predicted). $S_{ij} = 1$ if and only if  Z and Q agrees that the pair of observation ${x_i,x_j}$ belong to the same cluster, otherwise $S_{ij} = 0$. Similar  $D_{ij} = 1$ only if Z and Q agrees that the pair of observation ${x_i,x_j}$ doesn't belong to the same cluster. 
We then calculate the rand index like:
\begin{equation}
R(Q, P)=\frac{S+D}{\frac{1}{2} N(N-1)}
\end{equation}

\section{Results}

Neural network

With leave-one out cross-validation the estimated accuracy is 71\% with a 95\% confidence interval of  [79.9\% , 62.1\%]

\begin{table}[H]
\begin{tabular}{lllllllllll}
\hline
\textbf{Person}   & 1                        & 2                        & 3    & 4    & 5     & 6    & 7    & 8    & 9    & 10   \\ \hline
\textbf{Accuracy} & \multicolumn{1}{c}{50\%} & \multicolumn{1}{c}{60\%} & 60\% & 80\% & 100\% & 80\% & 70\% & 80\% & 70\% & 60\% \\ \hline
\end{tabular}
\caption{Neural network, Accuracy on each person}
\label{tab:nn}
\end{table}

\end{document}